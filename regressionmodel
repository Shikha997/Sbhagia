# importing libraries
import random            as rand                     # random number gen
import pandas            as pd                       # data science essentials
import matplotlib.pyplot as plt                      # data visualization
import seaborn           as sns                      # enhanced data viz
import numpy as np                                   # mathematical essentials
from sklearn.linear_model import LinearRegression    # linear regression (scikit-learn)
import sklearn.linear_model                          # linear models
from sklearn.model_selection import train_test_split # train-test split
from sklearn.linear_model import LogisticRegression  # logistic regression
import statsmodels.formula.api as smf                # logistic regression
from sklearn.metrics import confusion_matrix         # confusion matrix
from sklearn.metrics import roc_auc_score            # auc score
from sklearn.neighbors import KNeighborsClassifier   # KNN for classification
from sklearn.neighbors import KNeighborsRegressor    # KNN for regression
from sklearn.preprocessing import StandardScaler     # standard scaler
from sklearn.tree import DecisionTreeClassifier      # classification trees
from sklearn.tree import export_graphviz             # exports graphics
from six import StringIO                             # saves objects in memory
from IPython.display import Image                    # displays on frontend
import pydotplus                                     # interprets dot objects
from sklearn.model_selection import RandomizedSearchCV     # hyperparameter tuning
from sklearn.metrics import make_scorer              # customizable scorer
from sklearn.ensemble import RandomForestClassifier     # random forest
from sklearn.ensemble import GradientBoostingClassifier # gbm


# setting pandas print options
pd.set_option('display.max_rows', 500)
pd.set_option('display.max_columns', 500)
pd.set_option('display.width', 1000)

########################################
# plot_feature_importances
########################################
def plot_feature_importances(model, train, export = False):
    """
    Plots the importance of features from a CART model.
    
    PARAMETERS
    ----------
    model  : CART model
    train  : explanatory variable training data
    export : whether or not to export as a .png image, default False
    """
    
    # declaring the number
    n_features = x_train.shape[1]
    
    # setting plot window
    fig, ax = plt.subplots(figsize=(12,9))
    
    plt.barh(range(n_features), model.feature_importances_, align='center')
    plt.yticks(pd.np.arange(n_features), train.columns)
    plt.xlabel("Feature importance")
    plt.ylabel("Feature")
    
    if export == True:
        plt.savefig('Tree_Leaf_50_Feature_Importance.png')
        
    #importing dataset
file = 'C:/Users/14253/Documents/Apprentice_Chef_Dataset.xlsx'


# reading the file into Python
chef_dataset = pd.read_excel(file)

#display dataset
chef_dataset.head(n=5)

# descriptive statistics for numeric data
chef_dataset.describe(include = 'number').round(2)

# Splitting emails

# placeholder list
placeholder_lst = []

# looping over each email address
for index, col in chef_dataset.iterrows():
    
    # splitting email domain at '@'
    split_email = chef_dataset.loc[index, 'EMAIL'].split(sep = '@')
    #print(split_email)
    
    # appending placeholder_lst with the results
    placeholder_lst.append(split_email)
    

# converting placeholder_lst into a DataFrame 
email_df = pd.DataFrame(placeholder_lst)


# displaying the results
email_df.columns = ['NAME', 'EMAIL_DOMAIN']

#email_df["domain"].value_counts()

chef_dataset = pd.concat([chef_dataset, email_df['EMAIL_DOMAIN']], 
                       axis = 1)
                       
# email domain types
professional_email_domains = ['@mmm.com', '@amex.com', '@apple.com', 
                            '@boeing.com', '@caterpillar.com', 
                            '@chevron.com','@cisco.com', '@cocacola.com', 
                            '@disney.com', '@dupont.com', '@exxon.com', 
                            '@ge.org', '@goldmansacs.com', '@homedepot.com', 
                            '@ibm.com', '@intel.com', '@jnj.com', 
                            '@jpmorgan.com', '@mcdonalds.com', '@merck.com', 
                            '@microsoft.com','@nike.com', '@pfizer.com', 
                            '@pg.com', '@travelers.com', '@unitedtech.com', 
                            '@unitedhealth.com', '@verizon.com', '@visa.com', 
                            '@walmart.com']

personal_email_domains = ['@gmail.com', '@yahoo.com', '@protonmail.com']

junk_email_domains = ['@me.com', '@aol.com', '@hotmail.com', '@live.com', 
                      '@msn.com', '@passport.com']


# placeholder list
placeholder_lst = []

# looping to group observations by domain type
for domain in chef_dataset['EMAIL_DOMAIN']:
        if '@' + domain in professional_email_domains:
            placeholder_lst.append('PROFESSIONAL')
            
        elif '@' + domain in personal_email_domains:
            placeholder_lst.append('PERSONAL')
        
        elif '@' + domain in junk_email_domains:
            placeholder_lst.append('JUNK')
            
        else:
            print('Unknown')


# concatenating with original DataFrame
chef_dataset['DOMAIN_GROUP'] = pd.Series(placeholder_lst)  ## a series is a column in a dataframe


# checking results
chef_dataset['DOMAIN_GROUP'].value_counts()

# one hot encoding categorical variables
one_hot_email = pd.get_dummies(chef_dataset['DOMAIN_GROUP'])

# dropping categorical variables after they've been encoded
chef_dataset = chef_dataset.drop('DOMAIN_GROUP', axis = 1)

# Joining the two df together
chef_dataset = chef_dataset.join([one_hot_email])

# log transforming Sale_Price and saving it to the dataset
chef_dataset['log_revenue'] = np.log10(chef_dataset['REVENUE'])

# creating new variables 
chef_dataset['REVENUE_PRO'] = chef_dataset['REVENUE'] / chef_dataset['TOTAL_MEALS_ORDERED']
chef_dataset['DRINKS']= [1 if x > 23 else 0 for x in chef_dataset['REVENUE_PRO']]
chef_dataset['CUST_SERVICE']= chef_dataset['CONTACTS_W_CUSTOMER_SERVICE'] *100 / chef_dataset['TOTAL_MEALS_ORDERED']
chef_dataset['EARLY_DELIVERY']= [1 if x >= 1 else 0 for x in chef_dataset['EARLY_DELIVERIES']]
chef_dataset['LATE_DELIVERY']= [1 if x >= 1 else 0 for x in chef_dataset['LATE_DELIVERIES']]
chef_dataset['COUNT_NAME']= chef_dataset['NAME'].str.len()

#checking new variables in the datasset
chef_dataset.head(n=5)

#check correlation 
df_corr = chef_dataset.corr(method = 'pearson').round(decimals = 2)

df_corr['CROSS_SELL_SUCCESS'].sort_values(ascending = False)

# preparing explanatory variable data
chef_dataset_data   = chef_dataset.drop(['CROSS_SELL_SUCCESS'], axis = 1)
chef_dataset_target =  chef_dataset.loc[ : , 'CROSS_SELL_SUCCESS']


# train-test spliting
x_train, x_test, y_train, y_test = train_test_split(
            chef_dataset_data,
            chef_dataset_target,
            random_state = 219,
            test_size    = 0.25,
            stratify     = chef_dataset_target)

# merging training data for statsmodels
chef_dataset_train = pd.concat([x_train, y_train], axis = 1)

# All columns
new_columns = chef_dataset_data.columns

for i in new_columns:
    print(f"{i} +")
    
print(f"""

Response Variable Proportions (Training Set)
--------------------------------------------
{y_train.value_counts(normalize = True).round(decimals = 2)}



Response Variable Proportions (Testing Set)
--------------------------------------------
{y_test.value_counts(normalize = True).round(decimals = 2)}
""")

# instantiating a logistic regression model object
logitic_regression_full = smf.logit(formula = """ CROSS_SELL_SUCCESS ~ log_revenue +
                                                                        TOTAL_MEALS_ORDERED +
                                                                        UNIQUE_MEALS_PURCH +
                                                                        CONTACTS_W_CUSTOMER_SERVICE +
                                                                        PRODUCT_CATEGORIES_VIEWED +
                                                                        AVG_TIME_PER_SITE_VISIT +
                                                                        MOBILE_NUMBER +
                                                                        CANCELLATIONS_BEFORE_NOON +
                                                                        CANCELLATIONS_AFTER_NOON +
                                                                        TASTES_AND_PREFERENCES +
                                                                        PC_LOGINS +
                                                                        MOBILE_LOGINS +
                                                                        WEEKLY_PLAN +
                                                                        EARLY_DELIVERIES +
                                                                        LATE_DELIVERIES +
                                                                        PACKAGE_LOCKER +
                                                                        REFRIGERATED_LOCKER +
                                                                        AVG_PREP_VID_TIME +
                                                                        LARGEST_ORDER_SIZE +
                                                                        MASTER_CLASSES_ATTENDED +
                                                                        MEDIAN_MEAL_RATING +
                                                                        AVG_CLICKS_PER_VISIT +
                                                                        TOTAL_PHOTOS_VIEWED +
                                                                        JUNK +
                                                                        PERSONAL +
                                                                        PROFESSIONAL +
                                                                        REVENUE_PRO +
                                                                        DRINKS +
                                                                        CUST_SERVICE +
                                                                        EARLY_DELIVERY +
                                                                        LATE_DELIVERY +
                                                                        COUNT_NAME """,
                                         data    = chef_dataset)


# fitting the model object
results_full = logitic_regression_full.fit()


# checking the results SUMMARY
results_full.summary()

# x variables set
my_dict = {
 
 # full model
 'logit_reg_full'   : ['CROSS_SELL_SUCCESS', 'log_revenue', 'TOTAL_MEALS_ORDERED', 
                   'UNIQUE_MEALS_PURCH', 'CONTACTS_W_CUSTOMER_SERVICE', 
                   'PRODUCT_CATEGORIES_VIEWED', 'AVG_TIME_PER_SITE_VISIT', 
                   'MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', 'CANCELLATIONS_AFTER_NOON', 
                   'TASTES_AND_PREFERENCES', 'PC_LOGINS', 'MOBILE_LOGINS', 
                   'WEEKLY_PLAN', 'EARLY_DELIVERIES', 'LATE_DELIVERIES', 'PACKAGE_LOCKER', 
                   'REFRIGERATED_LOCKER', 'AVG_PREP_VID_TIME', 'LARGEST_ORDER_SIZE', 
                   'MASTER_CLASSES_ATTENDED', 'MEDIAN_MEAL_RATING', 'AVG_CLICKS_PER_VISIT', 
                   'TOTAL_PHOTOS_VIEWED', 'JUNK','PERSONAL', 'PROFESSIONAL','REVENUE_PRO',
                   'DRINKS', 'CUST_SERVICE','EARLY_DELIVERY', 'LATE_DELIVERY',
                   'COUNT_NAME'],
    
 # significant variables based on p values only
 'logit_reg_sig'    : ['MOBILE_NUMBER', 'CANCELLATIONS_BEFORE_NOON', 'JUNK','PERSONAL',
                   'CANCELLATIONS_AFTER_NOON', 'TASTES_AND_PREFERENCES', 'EARLY_DELIVERIES',
                   'COUNT_NAME', 'PC_LOGINS', 'MOBILE_LOGINS', 'REFRIGERATED_LOCKER',
                   'CONTACTS_W_CUSTOMER_SERVICE','log_revenue']

}
#EARLY_DELIVERIES

# train/test split with the logit_sig set
chef_dataset_data   =  chef_dataset.loc[ : , my_dict['logit_reg_sig']]
chef_dataset_target =  chef_dataset.loc[ : , 'CROSS_SELL_SUCCESS']


# train-test spliting
x_train, x_test, y_train, y_test = train_test_split(
            chef_dataset_data,
            chef_dataset_target,
            random_state = 219,
            test_size    = 0.25,
            stratify     = chef_dataset_target)
            
#LOGISTIC REGRESSION
# INSTANTIATING a logistic regression model
logreg = LogisticRegression(solver = "lbfgs",
                            C = 4,
                            random_state = 219)
# FITTING the training data
logreg_fit = logreg.fit(x_train, y_train)


# PREDICTING based on the testing set
logreg_pred = logreg_fit.predict(x_test)


# SCORING the results
print('Training ACCURACY:', logreg_fit.score(x_train, y_train).round(4))
print('Testing  ACCURACY:', logreg_fit.score(x_test, y_test).round(4))


# SCORING with AUC
print('AUC Score        :', roc_auc_score(y_true  = y_test,
                                          y_score = logreg_pred).round(4))


# saving scoring data for future use
logreg_train_score = logreg_fit.score(x_train, y_train).round(4) # accuracy
logreg_test_score  = logreg_fit.score(x_test, y_test).round(4)   # accuracy


# saving AUC score
logreg_auc_score = roc_auc_score(y_true  = y_test,
                                 y_score = logreg_pred).round(4)
                                 
# unpacking the confusion matrix
logreg_tn, \
logreg_fp, \
logreg_fn, \
logreg_tp = confusion_matrix(y_true = y_test, y_pred = logreg_pred).ravel()


# printing each result one-by-one
print(f"""
True Negatives : {logreg_tn}
False Positives: {logreg_fp}
False Negatives: {logreg_fn}
True Positives : {logreg_tp}
""")

# INSTANTIATING a classification tree object
full_tree = DecisionTreeClassifier()


# FITTING the training data
full_tree_fit = full_tree.fit(x_train, y_train)


# PREDICTING on new data
full_tree_pred = full_tree_fit.predict(x_test)


# SCORING the model
print('Full Tree Training ACCURACY:', full_tree_fit.score(x_train,
                                                    y_train).round(4))

print('Full Tree Testing ACCURACY :', full_tree_fit.score(x_test,
                                                    y_test).round(4))

print('Full Tree AUC Score:', roc_auc_score(y_true  = y_test,
                                            y_score = full_tree_pred).round(4))


# saving scoring data for future use
full_tree_train_score = full_tree_fit.score(x_train, y_train).round(4) # accuracy
full_tree_test_score  = full_tree_fit.score(x_test, y_test).round(4)   # accuracy


# saving AUC
full_tree_auc_score   = roc_auc_score(y_true  = y_test,
                                      y_score = full_tree_pred).round(4) # auc
                                      
                                      
# unpacking the confusion matrix
full_tree_tn, \
full_tree_fp, \
full_tree_fn, \
full_tree_tp = confusion_matrix(y_true = y_test, y_pred = full_tree_pred).ravel()


# printing each result one-by-one
print(f"""
True Negatives : {full_tree_tn}
False Positives: {full_tree_fp}
False Negatives: {full_tree_fn}
True Positives : {full_tree_tp}
""")
#RANDOM FOREST
# INSTANTIATING a random forest model with default values
rf_default = RandomForestClassifier(n_estimators     = 100,
                                    criterion        = 'gini',
                                    max_depth        = None,
                                    min_samples_leaf = 1,
                                    bootstrap        = True,
                                    warm_start       = False,
                                    random_state     = 219)

# FITTING the training data
rf_default_fit = rf_default.fit(x_train, y_train)


# PREDICTING based on the testing set
rf_default_fit_pred = rf_default_fit.predict(x_test)


# SCORING the results
print('Training ACCURACY:', rf_default_fit.score(x_train, y_train).round(4))
print('Testing  ACCURACY:', rf_default_fit.score(x_test, y_test).round(4))


# saving AUC score
print('AUC Score        :', roc_auc_score(y_true  = y_test,
                                          y_score = rf_default_fit_pred).round(4))

# unpacking the confusion matrix
forest_tn, \
forest_fp, \
forest_fn, \
forest_tp = confusion_matrix(y_true = y_test, y_pred = rf_default_fit_pred).ravel()


# printing each result one-by-one
print(f"""
True Negatives : {forest_tn}
False Positives: {forest_fp}
False Negatives: {forest_fn}
True Positives : {forest_tp}
""")

# plotting feature importances
plot_feature_importances(rf_default_fit,
                         train = x_train,
                         export = False)

# comparing results
print(f"""
Model         AUC Score      TN, FP, FN, TP
-----         ---------      --------------
Logistic      {logreg_auc_score}         {logreg_tn, logreg_fp, logreg_fn, logreg_tp}
""")


# creating a dictionary for model results
model_performance = {
    
    'Model Name'    : ['Logistic'],
           
    'AUC Score' : [logreg_auc_score],
    
    'Training Accuracy' : [logreg_train_score],
           
    'Testing Accuracy'  : [logreg_test_score],

    'Confusion Matrix'  : [(logreg_tn, logreg_fp, logreg_fn, logreg_tp)]}


# converting model_performance into a DataFrame
model_performance = pd.DataFrame(model_performance)


# sending model results to Excel
model_performance.to_excel('./classification_model_performance.xlsx',
                           index = False)

# declaring model performance objects
forest_train_acc = rf_default_fit.score(x_train, y_train).round(4)
forest_test_acc  = rf_default_fit.score(x_test, y_test).round(4)
forest_auc       = roc_auc_score(y_true  = y_test,
                                          y_score = rf_default_fit_pred).round(4)

# appending to model_performance
model_performance = model_performance.append(
                          {'Model Name'        : 'Random Forest',
                           'Training Accuracy' : forest_train_acc,
                           'Testing Accuracy'  : forest_test_acc,
                           'AUC Score'         : forest_auc,
                           'Confusion Matrix'  : (forest_tn,
                                                  forest_fp,
                                                  forest_fn,
                                                  forest_tp)},
                           ignore_index = True)
#Tunning Random Forest
# building a model based on hyperparameter tuning results

# copy/pasting in the best_estimator_ results
# to avoid running another RandomizedSearch
forest_tuned = RandomForestClassifier(bootstrap=True, ccp_alpha=0.0, class_weight=None,
                       criterion='entropy', max_depth=None, max_features='auto',
                       max_leaf_nodes=None, max_samples=None,
                       min_impurity_decrease=0.0, min_impurity_split=None,
                       min_samples_leaf=11, min_samples_split=2,
                       min_weight_fraction_leaf=0.0, n_estimators=350,
                       n_jobs=None, oob_score=False, random_state=219,
                       verbose=0, warm_start=True)

# FITTING the model object
forest_tuned_fit = forest_tuned.fit(chef_dataset_data, chef_dataset_target)


# PREDICTING based on the testing set
forest_tuned_pred = forest_tuned_fit.predict(x_test)


# SCORING the results
print('Forest Tuned Training ACCURACY:', forest_tuned.score(x_train, y_train).round(4))
print('Forest Tuned Testing  ACCURACY:', forest_tuned.score(x_test, y_test).round(4))
print('Forest Tuned AUC Score        :', roc_auc_score(y_true  = y_test,
                                                   y_score = forest_tuned_pred).round(4))


# saving scoring data for future use
forest_tuned_train_score = forest_tuned.score(x_train, y_train).round(4) # accuracy
forest_tuned_test_score  = forest_tuned.score(x_test, y_test).round(4)   # accuracy


# saving the AUC score
forest_tuned_auc = roc_auc_score(y_true  = y_test,
                                 y_score = forest_tuned_pred).round(4) # auc

# unpacking the confusion matrix
tuned_forest_tn, \
tuned_forest_fp, \
tuned_forest_fn, \
tuned_forest_tp = confusion_matrix(y_true = y_test, y_pred = forest_tuned_pred).ravel()


# printing each result one-by-one
print(f"""
True Negatives : {tuned_forest_tn}
False Positives: {tuned_forest_fp}
False Negatives: {tuned_forest_fn}
True Positives : {tuned_forest_tp}
""")

# INSTANTIATING the model object without hyperparameters
gbt = GradientBoostingClassifier(loss          = 'deviance',
                                              learning_rate = 0.1,
                                              n_estimators  = 100,
                                              criterion     = 'friedman_mse',
                                              max_depth     =3,
                                              warm_start    = False,
                                              random_state  = 219)

gbt.fit(chef_dataset_data, chef_dataset_target)

gbt_pred = gbt.predict(x_test)


# SCORING the results
print('Training Score:', gbt.score(x_train, y_train).round(4))
print('Testing Score:',  gbt.score(x_test, y_test).round(4))
print('AUC:',roc_auc_score(y_true  = y_test,
              y_score = gbt_pred).round(4))

# saving scoring data for future use
gbt_score_train = gbt.score(x_train, y_train).round(4)
gbt_score_test  = gbt.score(x_test, y_test).round(4)
gbt_score_auc = roc_auc_score(y_true  = y_test,
              y_score = gbt_pred).round(4)

# unpacking the confusion matrix
gbt_tn, \
gbt_fp, \
gbt_fn, \
gbt_tp = confusion_matrix(y_true = y_test, y_pred = gbt_pred).ravel()


# printing each result one-by-one
print(f"""
True Negatives : {gbt_tn}
False Positives: {gbt_fp}
False Negatives: {gbt_fn}
True Positives : {gbt_tp}
""")

# saving the results
model_performance = model_performance.append(
                          {'Model Name'        : 'Gradient Boosting',
                           'Training Accuracy' : gbt_score_train,
                           'Testing Accuracy'  : gbt_score_test,
                           'AUC Score'         : gbt_score_auc,
                           'Confusion Matrix'  : (gbt_tn,
                                                  gbt_fp,
                                                  gbt_fn,
                                                  gbt_tp)},
                           ignore_index = True)

# checking the results
model_performance

print("Gradient Boosting has the highest AUC score. This should be used for any further analysis.")
